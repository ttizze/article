# 最強文字起こしをWhisper+GPT補正で作ろうとしたけど、ElevenLabsだけで十分だった件

## はじめに

仏教法話の文字起こしをすることになった。

1〜2時間の音声データが1000本近くあり、1から手作業でやるのは無理だ。
ということでAIを使うことになり、仲間がWhisperで文字起こしをしてみたのだが、精度がいまいちだった。特に仏教用語の誤認識が多く、「釈尊」が「作詞」になっていたりした。

ということで、何らかの方法で文字起こしの精度を上げる必要があった。
そしてどうせやるなら最高のものを作りたいと思ってしまった。

以前からDeep Researchを文字起こしに応用できないかと考えていて、こういうパイプラインを構想していた。

1. まず文字起こしをする
2. 文字起こし結果からトピックや専門用語を抽出
3. Deep Researchでそのトピックを検索し、関連文献やWikipediaを収集
4. 専門用語集を自動生成（正式名称、読み方、文脈）
5. その用語集をコンテキストにしてLLMで補正
6. 更にマルチモーダルモデルに音声と補正データを投げて､より自然な文章に修正
7. 完璧な文字起こしの完成

仏教だけじゃない。医療、法律、学術、どんな専門分野でも対応できる汎用的な最強文字起こしシステム。これを作れば、世の中の文字起こしの悩みをすべて解決できる。

名付けて「Deep Transcribe」。

と、壮大な野望を抱いて検証を始めた。
結論から言うと､ElevenLabsだけで十分だった。

## パイプライン構築

もうすでにWhisperの出力が存在したので､僕の予想では､これを元にパイプラインを構築すればいい｡それで最強の文字起こしができるはずだ｡
実際のWhisperでの文字起こしデータと､正解データとの比較がこれだ｡
##TODO: Whisperでの文字起こしデータを追加

##TODO: 正解データを追加

見てわかるように､Whisperの精度はいまいちだ｡

ということで､パイプラインを構築してみる｡
パイプラインの構築はClaude Codeですぐできた｡
Whisperの出力を元に､GPT-4oでトピックや専門用語を抽出､Deep Researchで検索して､関連文献やWikipediaを収集して､専門用語集を自動生成､その用語集をコンテキストにしてGPT5.2で補正､さらにGPT4o-transcribeで音声と補正データを投げて､より自然な文章に修正､完璧な文字起こしの完成｡
結果はこれだ｡

##TODO: 全部完了後の結果


見ればわかるが､思ったように精度が出ていない｡

をGPT-4oで補正してみる。Gemini 2.5 Flashでも試す。用語集を渡して、仏教用語を正しく修正するよう指示する。

結果、大して変わらなかった。

むしろ、補正AIが「余計なお世話」をして、正しかった部分を間違った表現に直してしまうこともあった。元のSTTの精度が低いと、補正でカバーしきれない。かといって補正を強くすると、今度は正しい部分まで壊れる。

これは根本的にアプローチが間違っているのでは？と思い始めた。


## 文字起こしを変えた

補正でなんとかするのではなく、そもそものSTT（Speech-to-Text）を変えてみることにした。

試したのは以下の4つ。

- **Whisper (OpenAI)** - 元々使っていたもの。オープンソースで人気
- **Deepgram (Nova-2)** - 高速で安いと評判
- **ElevenLabs** - 音声合成で有名だけど、文字起こしAPIもある
- **Gemini 2.5 Flash** - Googleの最新マルチモーダルモデル

2時間45分の仏教法話を、それぞれのサービスで文字起こしした。

| サービス | 文字数 |
|---------|--------|
| ElevenLabs | 38,569 |
| Whisper | 37,233 |
| Deepgram | 34,885 |

文字数だけ見ると大差ないように見える。でも、実際に出力を並べて比較すると、その差は歴然だった。

ElevenLabsが圧倒的に自然な日本語を出力していた。

Whisperは句読点の位置が不自然だったり、同じフレーズが繰り返されたりすることがあった。Deepgramは文字数が少なめで、情報が欠落している可能性がある。Geminiは音声入力に対応しているものの、長時間音声の処理には向いていなかった。

一方、ElevenLabsの出力は、句読点の配置が自然で、文脈に応じた漢字変換ができていて、フィラー（「えー」「あの」など）の処理も適切だった。仏教用語の認識精度も、他と比べて明らかに高い。


## これでいいじゃん

ElevenLabsの出力をさらにGPT-4oで補正してみた。

結果、ほとんど変わらなかった。ElevenLabsの出力がすでに十分な精度だったので、補正する余地がほとんどなかったんだ。

Deep Research + 用語集自動生成 + AI補正という壮大なパイプライン。実装する前に、不要だと気づいてしまった。

考えてみれば当たり前のことだった。STT自体の精度が十分に高ければ、補正の余地がない。補正処理を挟めば挟むほど、処理時間は増え、コストは上がり、エラーの可能性も増える。そして複雑なシステムは保守も大変だ。

**「最高のモデルを使う」というシンプルな解が、最強だった。**

僕が学んだのは、**問題を複雑に解こうとする前に、まずシンプルな解を試せ**ということだった。

エンジニアはつい複雑なシステムを作りたがる。「これを自動化したら」「あれを組み合わせたら」と考えてしまう。でも、最高のツールを使うだけで解決する問題も多い。

今回の場合、僕がやるべきだったのは、最初から複数のSTTサービスを比較することだった。それだけで、壮大なパイプライン構想に費やした時間を節約できた。

まあ、こうして記事のネタになったので、良しとしよう。


## おまけ：料金比較

TODO: 各サービスの料金を調査して追記
